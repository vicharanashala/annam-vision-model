{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13154571,"sourceType":"datasetVersion","datasetId":8334656}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CREATING ORGANIZED TOMATO POTATO DATASET","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# --- CONFIGURATION ---\nSOURCE_DIRECTORY = '/kaggle/input/combined-dataset1to4-modified/Combined_Dataset1to4'\nOUTPUT_DIRECTORY = 'Organized_Dataset_Tomato_Potato' # New output folder\n\n# ❗ KEY CHANGE: Define a list of the only plant categories you want to process.\n# The script will ignore any folder that doesn't belong to these categories.\nCLASSES_TO_PROCESS = ['Tomato', 'Potato']\n\n# --- SCRIPT LOGIC ---\n\nPLANT_MAP = {\n    'Apple': 'Apple', 'Cotton': 'Cotton', 'Rice': 'Rice',\n    'Blueberry': 'Blueberry', 'Cherry': 'Cherry', 'Corn': 'Corn', 'Maize': 'Corn',\n    'Grape': 'Grape', 'Orange': 'Orange', 'Peach': 'Peach', 'Pepper': 'Pepper',\n    'Potato': 'Potato', 'Raspberry': 'Raspberry', 'Soybean': 'Soybean',\n    'Squash': 'Squash', 'Strawberry': 'Strawberry', 'Sugarcane': 'Sugarcane',\n    'Tomato': 'Tomato', 'Wheat': 'Wheat'\n}\n\ndef get_new_names(original_name):\n    \"\"\"Parses the original folder name to get the new category and subfolder name.\"\"\"\n    name_lower = original_name.lower()\n\n    if '_on_' in name_lower:\n        parts = original_name.split('_on_')\n        return parts[1].capitalize(), parts[0].replace('_', ' ')\n    if '_in_' in name_lower:\n        parts = original_name.split('_in_')\n        return parts[1].capitalize(), parts[0].replace('_', ' ')\n    \n    for keyword, category in PLANT_MAP.items():\n        if keyword.lower() in name_lower:\n            new_subfolder_name = original_name.replace(keyword, '').replace('___', '_').strip(' _')\n            new_subfolder_name = new_subfolder_name.replace('(maize)', '').replace('(including_sour)', '').strip(' _')\n            \n            if not new_subfolder_name or new_subfolder_name.lower() == 'healthy':\n                new_subfolder_name = 'healthy'\n                \n            return category, new_subfolder_name.replace('_', ' ').capitalize()\n\n    return \"Unclassified\", original_name\n\ndef main():\n    \"\"\"Main function to create the new structure and copy files.\"\"\"\n    print(f\"Preparing to organize folders for {CLASSES_TO_PROCESS}...\")\n    print(f\"Source: '{SOURCE_DIRECTORY}'\")\n\n    try:\n        all_folders = [f for f in os.listdir(SOURCE_DIRECTORY) if os.path.isdir(os.path.join(SOURCE_DIRECTORY, f))]\n        print(f\"Found {len(all_folders)} total folders to check.\")\n    except FileNotFoundError:\n        print(f\"❌ Error: The source directory was not found: '{SOURCE_DIRECTORY}'\")\n        return\n        \n    os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n    \n    processed_count = 0\n    for original_folder_name in all_folders:\n        source_path = os.path.join(SOURCE_DIRECTORY, original_folder_name)\n        \n        category, new_subfolder_name = get_new_names(original_folder_name)\n        \n        # ❗ KEY CHANGE: Check if the detected category is in our target list.\n        if category in CLASSES_TO_PROCESS:\n            # If it is, proceed with copying the folder.\n            destination_path = os.path.join(OUTPUT_DIRECTORY, category, new_subfolder_name)\n            \n            print(f\"Copying: '{original_folder_name}'  ->  '{category}/{new_subfolder_name}'\")\n            \n            try:\n                shutil.copytree(source_path, destination_path)\n                processed_count += 1\n            except FileExistsError:\n                print(f\"    - Skipped: Destination folder already exists.\")\n            except Exception as e:\n                print(f\"    - ❌ Error copying '{original_folder_name}': {e}\")\n        # If the category is not 'Tomato' or 'Potato', the script simply ignores it\n        # and moves to the next folder.\n            \n    print(f\"\\n✅ Done! Processed and copied {processed_count} folders related to {CLASSES_TO_PROCESS}.\")\n    print(f\"Your new, organized dataset is ready in the '{OUTPUT_DIRECTORY}' folder.\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create Stage 1 Splits (Plant Classification)","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\n# --- CONFIGURATION ---\n# The root directory of the previously organized dataset (Stage 0 output)\nSOURCE_ROOT = '/kaggle/working/Organized_Dataset_Tomato_Potato'\n# The root directory for the final Stage 1 (Coarse Classification) splits\nDESTINATION_ROOT = 'Stage_1_Splits'\n\n# Define the plant categories to process (must match the folder names in SOURCE_ROOT)\nPLANT_CATEGORIES = ['Tomato', 'Potato']\n\n# Define the desired split ratios (must sum to 1.0)\nSPLIT_RATIOS = {\n    'train': 0.70,\n    'validation': 0.15,\n    'test': 0.15\n}\n\ndef split_data():\n    \"\"\"\n    Combines all disease images per plant, shuffles them, and splits them\n    into the final train, validation, and test directories for Stage 1.\n    \"\"\"\n    if sum(SPLIT_RATIOS.values()) != 1.0:\n        print(\"❌ Error: Split ratios must sum exactly to 1.0. Check your configuration.\")\n        return\n\n    print(f\"Starting data split process for Stage 1 (Train: {SPLIT_RATIOS['train']:.0%}, Valid: {SPLIT_RATIOS['validation']:.0%}, Test: {SPLIT_RATIOS['test']:.0%})\")\n\n    # 1. Create the necessary destination directories\n    for split_type in SPLIT_RATIOS.keys():\n        for category in PLANT_CATEGORIES:\n            os.makedirs(os.path.join(DESTINATION_ROOT, split_type, category), exist_ok=True)\n\n    total_images_processed = 0\n\n    # 2. Process each plant category\n    for category in PLANT_CATEGORIES:\n        source_category_path = os.path.join(SOURCE_ROOT, category)\n\n        if not os.path.exists(source_category_path):\n            print(f\"⚠️ Warning: Source folder not found for {category} at {source_category_path}. Skipping.\")\n            continue\n\n        print(f\"\\n--- Processing {category} ---\")\n\n        # Collect all image file paths across all disease subfolders\n        all_image_paths = []\n        for root, _, files in os.walk(source_category_path):\n            for file in files:\n                # Basic check to ensure we only process image files\n                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                    all_image_paths.append(os.path.join(root, file))\n\n        # 3. Shuffle and split the paths\n        random.shuffle(all_image_paths)\n        total_count = len(all_image_paths)\n        print(f\"Total images found: {total_count}\")\n\n        if total_count == 0:\n             print(f\"Skipping {category}: No images found.\")\n             continue\n\n        # Calculate split indices\n        train_end = int(total_count * SPLIT_RATIOS['train'])\n        validation_end = train_end + int(total_count * SPLIT_RATIOS['validation'])\n\n        # Split the list\n        train_files = all_image_paths[:train_end]\n        validation_files = all_image_paths[train_end:validation_end]\n        # Test takes the remainder to ensure all files are used\n        test_files = all_image_paths[validation_end:] \n\n        # Store files to copy by split type\n        split_files = {\n            'train': train_files,\n            'validation': validation_files,\n            'test': test_files\n        }\n\n        # 4. Copy files to the final destination structure\n        for split_type, file_list in split_files.items():\n            destination_dir = os.path.join(DESTINATION_ROOT, split_type, category)\n            print(f\"  - Copying {len(file_list):5d} files to {split_type}/{category}...\")\n\n            for src_path in file_list:\n                # Use os.path.basename to get only the filename (flattening the disease structure)\n                dst_path = os.path.join(destination_dir, os.path.basename(src_path))\n                try:\n                    shutil.copy2(src_path, dst_path) # copy2 preserves metadata\n                    total_images_processed += 1\n                except Exception as e:\n                     print(f\"    - ❌ Error copying {os.path.basename(src_path)}: {e}\")\n\n\n    print(f\"\\n✅ Data splitting complete. Total images copied: {total_images_processed}\")\n    print(f\"The Stage 1 dataset is ready in the '{DESTINATION_ROOT}' folder.\")\n\nif __name__ == \"__main__\":\n    split_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:56:07.417609Z","iopub.execute_input":"2025-10-25T11:56:07.418290Z","iopub.status.idle":"2025-10-25T11:56:10.003031Z","shell.execute_reply.started":"2025-10-25T11:56:07.418254Z","shell.execute_reply":"2025-10-25T11:56:10.002262Z"}},"outputs":[{"name":"stdout","text":"Starting data split process for Stage 1 (Train: 70%, Valid: 15%, Test: 15%)\n\n--- Processing Tomato ---\nTotal images found: 19006\n  - Copying 13304 files to train/Tomato...\n  - Copying  2850 files to validation/Tomato...\n  - Copying  2852 files to test/Tomato...\n\n--- Processing Potato ---\nTotal images found: 2344\n  - Copying  1640 files to train/Potato...\n  - Copying   351 files to validation/Potato...\n  - Copying   353 files to test/Potato...\n\n✅ Data splitting complete. Total images copied: 21350\nThe Stage 1 dataset is ready in the 'Stage_1_Splits' folder.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Create Stage 2 Splits (Disease Classification)","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\n# --- CONFIGURATION ---\n# SOURCE_ROOT: Source Directory, which is the output from the initial organization script\nSOURCE_ROOT = '/kaggle/working/Organized_Dataset_Tomato_Potato'\n# DESTINATION_ROOT: Destination Directory for the final Stage 2 split dataset\nDESTINATION_ROOT = 'Stage_2_Splits'\n\n# Plant Categories to process (must match the top-level folders in SOURCE_ROOT)\nPLANT_CATEGORIES = ['Tomato', 'Potato']\n\n# Desired split ratios: Train / Validation / Test (must sum to 1.0)\nSPLIT_RATIOS = {\n    'train': 0.70,\n    'validation': 0.15,\n    'test': 0.15\n}\n\ndef split_disease_data():\n    \"\"\"\n    For each plant category, this script divides the data by disease into separate\n    train, validation, and test directories. This prepares the dataset for\n    the specialized Stage 2 models.\n    \"\"\"\n    if sum(SPLIT_RATIOS.values()) != 1.0:\n        print(\"❌ Error: Split ratios must sum exactly to 1.0. Please check the configuration.\")\n        return\n\n    print(f\"Starting data split process for Stage 2 (Train: {SPLIT_RATIOS['train']:.0%}, Validation: {SPLIT_RATIOS['validation']:.0%}, Test: {SPLIT_RATIOS['test']:.0%})\")\n\n    total_images_processed = 0\n\n    # 1. Loop through each plant category (e.g., 'Tomato', 'Potato')\n    for category in PLANT_CATEGORIES:\n        source_category_path = os.path.join(SOURCE_ROOT, category)\n\n        if not os.path.exists(source_category_path):\n            print(f\"⚠️ Warning: Source folder not found for {category} at {source_category_path}. Skipping.\")\n            continue\n\n        print(f\"\\n--- Processing {category} ---\")\n\n        # 2. Identify disease subfolders (which serve as the specific labels)\n        disease_folders = [d for d in os.listdir(source_category_path)\n                           if os.path.isdir(os.path.join(source_category_path, d))]\n\n        if not disease_folders:\n            print(f\"  - No disease subfolders found for {category}. Skipping.\")\n            continue\n\n        for disease_name in disease_folders:\n            source_disease_path = os.path.join(source_category_path, disease_name)\n\n            # 3. Collect all image file paths for this disease\n            all_image_paths = [os.path.join(source_disease_path, f)\n                               for f in os.listdir(source_disease_path)\n                               if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n\n            random.shuffle(all_image_paths)\n            total_count = len(all_image_paths)\n\n            if total_count == 0:\n                 print(f\"  - No images found for disease '{disease_name}'. Skipping.\")\n                 continue\n\n            print(f\"  - Disease '{disease_name}': Total {total_count} images.\")\n\n            # 4. Calculate Split Indices\n            train_end = int(total_count * SPLIT_RATIOS['train'])\n            validation_end = train_end + int(total_count * SPLIT_RATIOS['validation'])\n\n            train_files = all_image_paths[:train_end]\n            validation_files = all_image_paths[train_end:validation_end]\n             # Test takes the remainder to ensure all files are used\n            test_files = all_image_paths[validation_end:]\n\n            split_files = {\n                'train': train_files,\n                'validation': validation_files,\n                'test': test_files\n            }\n\n            # 5. Copy files to the destination structure\n            for split_type, file_list in split_files.items():\n                # Destination Path: DESTINATION_ROOT / PLANT / SPLIT_TYPE / DISEASE\n                destination_dir = os.path.join(DESTINATION_ROOT, category, split_type, disease_name)\n                os.makedirs(destination_dir, exist_ok=True)\n\n                print(f\"    - Copying {len(file_list):4d} images to {split_type}/{disease_name}...\")\n\n                for src_path in file_list:\n                    dst_path = os.path.join(destination_dir, os.path.basename(src_path))\n                    try:\n                        shutil.copy2(src_path, dst_path)\n                        total_images_processed += 1\n                    except Exception as e:\n                        print(f\"      - ❌ Error copying {os.path.basename(src_path)}: {e}\")\n\n\n    print(f\"\\n✅ Data splitting complete. Total images copied: {total_images_processed}\")\n    print(f\"The Stage 2 dataset is ready in the '{DESTINATION_ROOT}' folder.\")\n\nif __name__ == \"__main__\":\n    split_disease_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:57:05.861719Z","iopub.execute_input":"2025-10-25T11:57:05.862462Z","iopub.status.idle":"2025-10-25T11:57:08.410724Z","shell.execute_reply.started":"2025-10-25T11:57:05.862435Z","shell.execute_reply":"2025-10-25T11:57:08.410016Z"}},"outputs":[{"name":"stdout","text":"Starting data split process for Stage 2 (Train: 70%, Validation: 15%, Test: 15%)\n\n--- Processing Tomato ---\n  - Disease 'Yellow leaf curl virus': Total 5423 images.\n    - Copying 3796 images to train/Yellow leaf curl virus...\n    - Copying  813 images to validation/Yellow leaf curl virus...\n    - Copying  814 images to test/Yellow leaf curl virus...\n  - Disease 'Late blight': Total 2009 images.\n    - Copying 1406 images to train/Late blight...\n    - Copying  301 images to validation/Late blight...\n    - Copying  302 images to test/Late blight...\n  - Disease 'Healthy': Total 1684 images.\n    - Copying 1178 images to train/Healthy...\n    - Copying  252 images to validation/Healthy...\n    - Copying  254 images to test/Healthy...\n  - Disease 'Leaf mold': Total 1061 images.\n    - Copying  742 images to train/Leaf mold...\n    - Copying  159 images to validation/Leaf mold...\n    - Copying  160 images to test/Leaf mold...\n  - Disease 'Target spot': Total 1422 images.\n    - Copying  995 images to train/Target spot...\n    - Copying  213 images to validation/Target spot...\n    - Copying  214 images to test/Target spot...\n  - Disease 'Early blight': Total 1105 images.\n    - Copying  773 images to train/Early blight...\n    - Copying  165 images to validation/Early blight...\n    - Copying  167 images to test/Early blight...\n  - Disease 'Spider mites two-spotted spider mite': Total 1676 images.\n    - Copying 1173 images to train/Spider mites two-spotted spider mite...\n    - Copying  251 images to validation/Spider mites two-spotted spider mite...\n    - Copying  252 images to test/Spider mites two-spotted spider mite...\n  - Disease 'Septoria leaf spot': Total 1940 images.\n    - Copying 1358 images to train/Septoria leaf spot...\n    - Copying  291 images to validation/Septoria leaf spot...\n    - Copying  291 images to test/Septoria leaf spot...\n  - Disease 'Mosaic virus': Total 452 images.\n    - Copying  316 images to train/Mosaic virus...\n    - Copying   67 images to validation/Mosaic virus...\n    - Copying   69 images to test/Mosaic virus...\n  - Disease 'Bacterial spot': Total 2234 images.\n    - Copying 1563 images to train/Bacterial spot...\n    - Copying  335 images to validation/Bacterial spot...\n    - Copying  336 images to test/Bacterial spot...\n\n--- Processing Potato ---\n  - Disease 'Late blight': Total 1095 images.\n    - Copying  766 images to train/Late blight...\n    - Copying  164 images to validation/Late blight...\n    - Copying  165 images to test/Late blight...\n  - Disease 'Healthy': Total 152 images.\n    - Copying  106 images to train/Healthy...\n    - Copying   22 images to validation/Healthy...\n    - Copying   24 images to test/Healthy...\n  - Disease 'Early blight': Total 1097 images.\n    - Copying  767 images to train/Early blight...\n    - Copying  164 images to validation/Early blight...\n    - Copying  166 images to test/Early blight...\n\n✅ Data splitting complete. Total images copied: 21350\nThe Stage 2 dataset is ready in the 'Stage_2_Splits' folder.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install -U transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:57:38.807587Z","iopub.execute_input":"2025-10-25T11:57:38.808280Z","iopub.status.idle":"2025-10-25T11:57:52.841511Z","shell.execute_reply.started":"2025-10-25T11:57:38.808256Z","shell.execute_reply":"2025-10-25T11:57:52.840802Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 tokenizers-0.22.1 transformers-4.57.1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(new_session=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:58:05.519389Z","iopub.execute_input":"2025-10-25T11:58:05.519672Z","iopub.status.idle":"2025-10-25T11:58:05.780600Z","shell.execute_reply.started":"2025-10-25T11:58:05.519644Z","shell.execute_reply":"2025-10-25T11:58:05.779879Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38d77be4c5f949f18cfaba794aff3cae"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoModel\nimport torch, torchvision.transforms as T\nfrom PIL import Image\nimport requests\n\nMODEL_ID = \"facebook/dinov3-convnext-base-pretrain-lvd1689m\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# load model\nmodel = AutoModel.from_pretrained(MODEL_ID, trust_remote_code=True).to(DEVICE).eval()\n\n# preprocessing\ntransform = T.Compose([\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n])\n\n# test image\nurl = \"https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\"\nimg = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\nx = transform(img).unsqueeze(0).to(DEVICE)\n\nwith torch.no_grad():\n    out = model(pixel_values=x)\n\nprint(\"CLS embedding:\", out.last_hidden_state[:,0,:].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:58:14.287192Z","iopub.execute_input":"2025-10-25T11:58:14.287690Z","iopub.status.idle":"2025-10-25T11:58:41.038387Z","shell.execute_reply.started":"2025-10-25T11:58:14.287666Z","shell.execute_reply":"2025-10-25T11:58:41.037724Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/449 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed9bbfa04d0a4ad8bc1d5b4d4c5d8c8f"}},"metadata":{}},{"name":"stderr","text":"2025-10-25 11:58:25.525652: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761393505.735279      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761393505.792216      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/350M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ee96a77d994696a451baf7b194f4cd"}},"metadata":{}},{"name":"stdout","text":"CLS embedding: torch.Size([1, 1024])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# STAGE 1 TRAINING","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np, json, torch\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom transformers import AutoModel\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nfrom tqdm.auto import tqdm\n\n# -------- CONFIG: adjust if needed --------\nSPLIT_ROOT = Path(\"/kaggle/working/Stage_1_Splits\")\nOUT = Path(\"/kaggle/working/embeddings\"); OUT.mkdir(parents=True, exist_ok=True)\nBATCH_SIZE = 8\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_ID = \"facebook/dinov3-convnext-base-pretrain-lvd1689m\"\nIMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n# ------------------------------------------\n\n# HF auth (must have HF_TOKEN in Kaggle Secrets)\ntry:\n    hf_token = UserSecretsClient().get_secret(\"HF_TOKEN\")\n    login(token=hf_token, new_session=False)\nexcept:\n    print(\"HF_TOKEN not found in Kaggle Secrets. Proceeding without login, which might fail for some models.\")\n    hf_token=None\n\n# load model\nmodel = AutoModel.from_pretrained(MODEL_ID, trust_remote_code=True, token=hf_token).to(DEVICE).eval()\n\ntransform = T.Compose([\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n])\n\n# Collect class list from the 'train' subdirectory\ntrain_dir = SPLIT_ROOT / \"train\"\nif not train_dir.exists():\n    raise SystemExit(f\"The directory {train_dir} does not exist. Please check your file structure.\")\n\n# --- MODIFICATION START ---\n# Original line that finds all folders:\n# classes = sorted([p.name for p in train_dir.iterdir() if p.is_dir()])\n\n# New line to only use specific folders:\nclasses = [\"Potato\", \"Tomato\"]\n# --- MODIFICATION END ---\n\nif not classes:\n    raise SystemExit(f\"No class folders found in {train_dir}. Expected structure: {SPLIT_ROOT}/train/<CLASS>\")\n\ncls2idx = {c:i for i,c in enumerate(classes)}\nprint(\"Processing only these classes:\", len(classes), classes)\n\ndef collect_from_split(split_root, split_name):\n    \"\"\"Return (paths, labels) for the given split.\n       Expects structure: split_root/<split_name>/<CLASS>/*.jpg\n    \"\"\"\n    paths, labels = [], []\n    split_folder = split_root / split_name\n    if not split_folder.exists() or not split_folder.is_dir():\n        return paths, np.array(labels, dtype=np.int32)\n\n    for c in classes:\n        class_folder = split_folder / c\n        if not class_folder.exists() or not class_folder.is_dir():\n            continue\n        for f in sorted(class_folder.iterdir()):\n            if f.is_file() and f.suffix.lower() in IMAGE_EXTS:\n                paths.append(str(f))\n                labels.append(cls2idx[c])\n    return paths, np.array(labels, dtype=np.int32)\n\ntrain_paths, y_train = collect_from_split(SPLIT_ROOT, \"train\")\nval_paths,   y_val   = collect_from_split(SPLIT_ROOT, \"validation\")\ntest_paths,  y_test  = collect_from_split(SPLIT_ROOT, \"test\")\n\nprint(f\"Images — train: {len(train_paths)}, val: {len(val_paths)}, test: {len(test_paths)}\")\n\ndef batch_embed(paths, batch_size=BATCH_SIZE):\n    embs = []\n    model.eval()\n    for i in tqdm(range(0, len(paths), batch_size), desc=\"Embedding batches\"):\n        batch = paths[i:i+batch_size]\n        imgs = []\n        for p in batch:\n            try:\n                imgs.append(transform(Image.open(p).convert(\"RGB\")))\n            except Exception as e:\n                print(\"skip:\", p, \"err:\", e)\n        if not imgs:\n            continue\n        x = torch.stack(imgs).to(DEVICE)\n        with torch.no_grad():\n            out = model(pixel_values=x)\n        embs.append(out.last_hidden_state[:,0,:].cpu().numpy())\n    return np.vstack(embs) if embs else np.zeros((0,0), dtype=np.float32)\n\n# Create embeddings if they don't exist\nif (OUT/\"X_train.npy\").exists() and (OUT/\"X_val.npy\").exists() and (OUT/\"X_test.npy\").exists():\n    print(\"Embeddings already exist — loading.\")\n    X_train = np.load(OUT/\"X_train.npy\"); y_train = np.load(OUT/\"y_train.npy\")\n    X_val   = np.load(OUT/\"X_val.npy\");   y_val   = np.load(OUT/\"y_val.npy\")\n    X_test  = np.load(OUT/\"X_test.npy\");  y_test  = np.load(OUT/\"y_test.npy\")\nelse:\n    print(\"Extracting train embeddings...\")\n    X_train = batch_embed(train_paths)\n    np.save(OUT/\"X_train.npy\", X_train); np.save(OUT/\"y_train.npy\", y_train)\n    print(\"Saved X_train\", X_train.shape)\n\n    print(\"Extracting val embeddings...\")\n    X_val = batch_embed(val_paths)\n    np.save(OUT/\"X_val.npy\", X_val); np.save(OUT/\"y_val.npy\", y_val)\n    print(\"Saved X_val\", X_val.shape)\n\n    print(\"Extracting test embeddings...\")\n    X_test = batch_embed(test_paths)\n    np.save(OUT/\"X_test.npy\", X_test); np.save(OUT/\"y_test.npy\", y_test)\n    print(\"Saved X_test\", X_test.shape)\n\nprint(\"Done. Shapes:\", np.load(OUT/\"X_train.npy\").shape, np.load(OUT/\"X_val.npy\").shape, np.load(OUT/\"X_test.npy\").shape)\nprint(\"Embeddings saved to:\", OUT.resolve())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T12:06:54.867588Z","iopub.execute_input":"2025-10-25T12:06:54.867893Z","iopub.status.idle":"2025-10-25T12:13:45.728649Z","shell.execute_reply.started":"2025-10-25T12:06:54.867870Z","shell.execute_reply":"2025-10-25T12:13:45.728016Z"}},"outputs":[{"name":"stdout","text":"Processing only these classes: 2 ['Potato', 'Tomato']\nImages — train: 14933, val: 3201, test: 3205\nExtracting train embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Embedding batches:   0%|          | 0/1867 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ca421555122449abc959506019b0bb3"}},"metadata":{}},{"name":"stdout","text":"Saved X_train (14933, 768)\nExtracting val embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Embedding batches:   0%|          | 0/401 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e5a2748878b44b1b6689b962405e1b5"}},"metadata":{}},{"name":"stdout","text":"Saved X_val (3201, 768)\nExtracting test embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Embedding batches:   0%|          | 0/401 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c327a7980214f67bb5883dc108d2f56"}},"metadata":{}},{"name":"stdout","text":"Saved X_test (3205, 768)\nDone. Shapes: (14933, 768) (3201, 768) (3205, 768)\nEmbeddings saved to: /kaggle/working/embeddings\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, normalize\nimport joblib\nimport os\n\nEMB_DIR = \"/kaggle/working/embeddings\"\nMODEL_OUT = os.path.join(EMB_DIR, \"stage1_classifier_model.joblib\")\n\n\n# load\nX_train = np.load(os.path.join(EMB_DIR, \"X_train.npy\"))\ny_train = np.load(os.path.join(EMB_DIR, \"y_train.npy\"))\nX_val   = np.load(os.path.join(EMB_DIR, \"X_val.npy\"))\ny_val   = np.load(os.path.join(EMB_DIR, \"y_val.npy\"))\n\nprint(\"Shapes loaded — X_train:\", X_train.shape, \"y_train:\", y_train.shape, \"X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n\n# basic sanity\nif X_train.size == 0 or X_val.size == 0:\n    raise SystemExit(\"Empty embeddings — check previous step. Exiting.\")\n\nif X_train.shape[0] != y_train.shape[0] or X_val.shape[0] != y_val.shape[0]:\n    raise SystemExit(\"Mismatch between number of embeddings and labels. Exiting.\")\n\n# Option A: Standardize (common). with_mean=True normally okay unless huge memmap / sparse\nsc = StandardScaler(with_mean=True, with_std=True)\nX_train_s = sc.fit_transform(X_train)\nX_val_s   = sc.transform(X_val)\n\n# Option B (alternative, often good for cosine-like): L2 normalize\n# X_train_s = normalize(X_train, norm='l2')\n# X_val_s   = normalize(X_val, norm='l2')\n\n# Train logistic regression (use class_weight='balanced' if classes imbalanced)\nclf = LogisticRegression(max_iter=2000, n_jobs=-1, C=1.0, class_weight=None, random_state=42)\nclf.fit(X_train_s, y_train)\n\n# Predict & evaluate\ny_pred = clf.predict(X_val_s)\nacc = accuracy_score(y_val, y_pred)\nprint(f\"Validation accuracy: {acc:.4f}\\n\")\nprint(\"Classification report:\\n\", classification_report(y_val, y_pred))\n\n# Save scaler + model\njoblib.dump((sc, clf), MODEL_OUT)\nprint(\"Saved model to:\", MODEL_OUT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T12:16:32.644709Z","iopub.execute_input":"2025-10-25T12:16:32.645442Z","iopub.status.idle":"2025-10-25T12:16:37.695119Z","shell.execute_reply.started":"2025-10-25T12:16:32.645417Z","shell.execute_reply":"2025-10-25T12:16:37.694416Z"}},"outputs":[{"name":"stdout","text":"Shapes loaded — X_train: (14933, 768) y_train: (14933,) X_val: (3201, 768) y_val: (3201,)\nValidation accuracy: 0.9941\n\nClassification report:\n               precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97       351\n           1       1.00      1.00      1.00      2850\n\n    accuracy                           0.99      3201\n   macro avg       0.98      0.99      0.98      3201\nweighted avg       0.99      0.99      0.99      3201\n\nSaved model to: /kaggle/working/embeddings/stage1_classifier_model.joblib\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# STAGE 2 TOMATO ","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np, torch\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom transformers import AutoModel\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nfrom tqdm.auto import tqdm\n\n# -------- CONFIG --------\nSPLIT_ROOT = Path(\"/kaggle/working/Stage_2_Splits/Tomato\")\nOUT = Path(\"/kaggle/working/embeddings_tomato\"); OUT.mkdir(parents=True, exist_ok=True)\nBATCH_SIZE = 8\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_ID = \"facebook/dinov2-base\"\nIMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n# -------------------------\n\n# Hugging Face auth\ntry:\n    hf_token = UserSecretsClient().get_secret(\"HF_TOKEN\")\n    login(token=hf_token, new_session=False)\nexcept:\n    print(\"HF_TOKEN not found in Kaggle Secrets — proceeding without it.\")\n    hf_token = None\n\n# Load pretrained DINOv2 model\nmodel = AutoModel.from_pretrained(MODEL_ID, trust_remote_code=True, token=hf_token).to(DEVICE).eval()\n\n# Image transform\ntransform = T.Compose([\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n])\n\n# ---------------------------------------------------------------\n# Collect class folders under Tomato/train\n# ---------------------------------------------------------------\ntrain_dir = SPLIT_ROOT / \"train\"\nif not train_dir.exists():\n    raise SystemExit(f\"The directory {train_dir} does not exist.\")\n\nclasses = sorted([p.name for p in train_dir.iterdir() if p.is_dir()])\nif not classes:\n    raise SystemExit(\"No disease folders found inside Tomato/train.\")\n\ncls2idx = {c:i for i,c in enumerate(classes)}\nprint(f\"Found {len(classes)} Tomato disease classes:\\n\", classes)\n\n# ---------------------------------------------------------------\n# Collect image paths and labels for each split\n# ---------------------------------------------------------------\ndef collect_from_split(split_root, split_name):\n    paths, labels = [], []\n    split_folder = split_root / split_name\n    if not split_folder.exists():\n        return paths, np.array(labels, dtype=np.int32)\n\n    for c in classes:\n        class_folder = split_folder / c\n        if not class_folder.exists():\n            continue\n        for f in sorted(class_folder.iterdir()):\n            if f.is_file() and f.suffix.lower() in IMAGE_EXTS:\n                paths.append(str(f))\n                labels.append(cls2idx[c])\n    return paths, np.array(labels, dtype=np.int32)\n\ntrain_paths, y_train = collect_from_split(SPLIT_ROOT, \"train\")\nval_paths,   y_val   = collect_from_split(SPLIT_ROOT, \"validation\")\ntest_paths,  y_test  = collect_from_split(SPLIT_ROOT, \"test\")\n\nprint(f\"Images — train: {len(train_paths)}, val: {len(val_paths)}, test: {len(test_paths)}\")\n\n# ---------------------------------------------------------------\n# Embedding extraction\n# ---------------------------------------------------------------\ndef batch_embed(paths, batch_size=BATCH_SIZE):\n    embs = []\n    model.eval()\n    for i in tqdm(range(0, len(paths), batch_size), desc=\"Embedding batches\"):\n        batch = paths[i:i+batch_size]\n        imgs = []\n        for p in batch:\n            try:\n                imgs.append(transform(Image.open(p).convert(\"RGB\")))\n            except Exception as e:\n                print(\"skip:\", p, \"err:\", e)\n        if not imgs:\n            continue\n        x = torch.stack(imgs).to(DEVICE)\n        with torch.no_grad():\n            out = model(pixel_values=x)\n        embs.append(out.last_hidden_state[:,0,:].cpu().numpy())\n    return np.vstack(embs) if embs else np.zeros((0,0), dtype=np.float32)\n\n# ---------------------------------------------------------------\n# Save embeddings\n# ---------------------------------------------------------------\nif (OUT/\"X_train.npy\").exists():\n    print(\"Embeddings already exist — loading.\")\n    X_train = np.load(OUT/\"X_train.npy\"); y_train = np.load(OUT/\"y_train.npy\")\n    X_val   = np.load(OUT/\"X_val.npy\");   y_val   = np.load(OUT/\"y_val.npy\")\n    X_test  = np.load(OUT/\"X_test.npy\");  y_test  = np.load(OUT/\"y_test.npy\")\nelse:\n    print(\"Extracting train embeddings...\")\n    X_train = batch_embed(train_paths)\n    np.save(OUT/\"X_train.npy\", X_train); np.save(OUT/\"y_train.npy\", y_train)\n    print(\"Saved X_train\", X_train.shape)\n\n    print(\"Extracting val embeddings...\")\n    X_val = batch_embed(val_paths)\n    np.save(OUT/\"X_val.npy\", X_val); np.save(OUT/\"y_val.npy\", y_val)\n    print(\"Saved X_val\", X_val.shape)\n\n    print(\"Extracting test embeddings...\")\n    X_test = batch_embed(test_paths)\n    np.save(OUT/\"X_test.npy\", X_test); np.save(OUT/\"y_test.npy\", y_test)\n    print(\"Saved X_test\", X_test.shape)\n\nprint(\"Done. Shapes:\")\nprint(\"  Train:\", np.load(OUT/'X_train.npy').shape)\nprint(\"  Val:  \", np.load(OUT/'X_val.npy').shape)\nprint(\"  Test: \", np.load(OUT/'X_test.npy').shape)\nprint(\"Tomato embeddings saved to:\", OUT.resolve())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T12:20:09.673065Z","iopub.execute_input":"2025-10-25T12:20:09.673340Z","iopub.status.idle":"2025-10-25T12:26:17.334196Z","shell.execute_reply.started":"2025-10-25T12:20:09.673322Z","shell.execute_reply":"2025-10-25T12:26:17.333409Z"}},"outputs":[{"name":"stdout","text":"Found 10 Tomato disease classes:\n ['Bacterial spot', 'Early blight', 'Healthy', 'Late blight', 'Leaf mold', 'Mosaic virus', 'Septoria leaf spot', 'Spider mites two-spotted spider mite', 'Target spot', 'Yellow leaf curl virus']\nImages — train: 13300, val: 2847, test: 2859\nExtracting train embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Embedding batches:   0%|          | 0/1663 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ee0e16ddbf04d6da9a23f97653ccae2"}},"metadata":{}},{"name":"stdout","text":"Saved X_train (13300, 768)\nExtracting val embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Embedding batches:   0%|          | 0/356 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3afd0cbfdaa4106ac2240d74032fc6c"}},"metadata":{}},{"name":"stdout","text":"Saved X_val (2847, 768)\nExtracting test embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Embedding batches:   0%|          | 0/358 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"117f038a4a6c43a7b705679a64a6ff74"}},"metadata":{}},{"name":"stdout","text":"Saved X_test (2859, 768)\nDone. Shapes:\n  Train: (13300, 768)\n  Val:   (2847, 768)\n  Test:  (2859, 768)\nTomato embeddings saved to: /kaggle/working/embeddings_tomato\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, normalize\nimport joblib\nimport os\n\nEMB_DIR = \"/kaggle/working/embeddings_tomato\"\nMODEL_OUT = os.path.join(EMB_DIR, \"stage2_classifier_model_tomato.joblib\")\n\n\n# load\nX_train = np.load(os.path.join(EMB_DIR, \"X_train.npy\"))\ny_train = np.load(os.path.join(EMB_DIR, \"y_train.npy\"))\nX_val   = np.load(os.path.join(EMB_DIR, \"X_val.npy\"))\ny_val   = np.load(os.path.join(EMB_DIR, \"y_val.npy\"))\n\nprint(\"Shapes loaded — X_train:\", X_train.shape, \"y_train:\", y_train.shape, \"X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n\n# basic sanity\nif X_train.size == 0 or X_val.size == 0:\n    raise SystemExit(\"Empty embeddings — check previous step. Exiting.\")\n\nif X_train.shape[0] != y_train.shape[0] or X_val.shape[0] != y_val.shape[0]:\n    raise SystemExit(\"Mismatch between number of embeddings and labels. Exiting.\")\n\n# Option A: Standardize (common). with_mean=True normally okay unless huge memmap / sparse\nsc = StandardScaler(with_mean=True, with_std=True)\nX_train_s = sc.fit_transform(X_train)\nX_val_s   = sc.transform(X_val)\n\n# Option B (alternative, often good for cosine-like): L2 normalize\n# X_train_s = normalize(X_train, norm='l2')\n# X_val_s   = normalize(X_val, norm='l2')\n\n# Train logistic regression (use class_weight='balanced' if classes imbalanced)\nclf = LogisticRegression(max_iter=2000, n_jobs=-1, C=1.0, class_weight=None, random_state=42)\nclf.fit(X_train_s, y_train)\n\n# Predict & evaluate\ny_pred = clf.predict(X_val_s)\nacc = accuracy_score(y_val, y_pred)\nprint(f\"Validation accuracy: {acc:.4f}\\n\")\nprint(\"Classification report:\\n\", classification_report(y_val, y_pred))\n\n# Save scaler + model\njoblib.dump((sc, clf), MODEL_OUT)\nprint(\"Saved model to:\", MODEL_OUT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T12:27:06.847601Z","iopub.execute_input":"2025-10-25T12:27:06.848396Z","iopub.status.idle":"2025-10-25T12:27:28.628813Z","shell.execute_reply.started":"2025-10-25T12:27:06.848370Z","shell.execute_reply":"2025-10-25T12:27:28.627928Z"}},"outputs":[{"name":"stdout","text":"Shapes loaded — X_train: (13300, 768) y_train: (13300,) X_val: (2847, 768) y_val: (2847,)\nValidation accuracy: 0.9691\n\nClassification report:\n               precision    recall  f1-score   support\n\n           0       0.95      0.96      0.95       335\n           1       0.89      0.88      0.88       165\n           2       0.98      0.98      0.98       252\n           3       0.96      0.96      0.96       301\n           4       0.97      0.96      0.97       159\n           5       0.94      0.97      0.96        67\n           6       0.96      0.92      0.94       291\n           7       0.98      1.00      0.99       251\n           8       0.97      0.98      0.97       213\n           9       1.00      1.00      1.00       813\n\n    accuracy                           0.97      2847\n   macro avg       0.96      0.96      0.96      2847\nweighted avg       0.97      0.97      0.97      2847\n\nSaved model to: /kaggle/working/embeddings_tomato/stage2_classifier_model_tomato.joblib\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# STAGE 2 POTATO","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np, torch\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom transformers import AutoModel\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nfrom tqdm.auto import tqdm\n\n# -------- CONFIG --------\nSPLIT_ROOT = Path(\"/kaggle/working/Stage_2_Splits/Potato\")\nOUT = Path(\"/kaggle/working/embeddings_potato\"); OUT.mkdir(parents=True, exist_ok=True)\nBATCH_SIZE = 8\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_ID = \"facebook/dinov2-base\"\nIMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n# -------------------------\n\n# Hugging Face auth\ntry:\n    hf_token = UserSecretsClient().get_secret(\"HF_TOKEN\")\n    login(token=hf_token, new_session=False)\nexcept:\n    print(\"HF_TOKEN not found in Kaggle Secrets — proceeding without it.\")\n    hf_token = None\n\n# Load pretrained DINOv2 model\nmodel = AutoModel.from_pretrained(MODEL_ID, trust_remote_code=True, token=hf_token).to(DEVICE).eval()\n\n# Image transform\ntransform = T.Compose([\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n])\n\n# ---------------------------------------------------------------\n# Collect class folders under Potato/train\n# ---------------------------------------------------------------\ntrain_dir = SPLIT_ROOT / \"train\"\nif not train_dir.exists():\n    raise SystemExit(f\"The directory {train_dir} does not exist.\")\n\nclasses = sorted([p.name for p in train_dir.iterdir() if p.is_dir()])\nif not classes:\n    raise SystemExit(\"No disease folders found inside Potato/train.\")\n\ncls2idx = {c:i for i,c in enumerate(classes)}\nprint(f\"Found {len(classes)} Potato disease classes:\\n\", classes)\n\n# ---------------------------------------------------------------\n# Collect image paths and labels for each split\n# ---------------------------------------------------------------\ndef collect_from_split(split_root, split_name):\n    paths, labels = [], []\n    split_folder = split_root / split_name\n    if not split_folder.exists():\n        return paths, np.array(labels, dtype=np.int32)\n\n    for c in classes:\n        class_folder = split_folder / c\n        if not class_folder.exists():\n            continue\n        for f in sorted(class_folder.iterdir()):\n            if f.is_file() and f.suffix.lower() in IMAGE_EXTS:\n                paths.append(str(f))\n                labels.append(cls2idx[c])\n    return paths, np.array(labels, dtype=np.int32)\n\ntrain_paths, y_train = collect_from_split(SPLIT_ROOT, \"train\")\nval_paths,   y_val   = collect_from_split(SPLIT_ROOT, \"validation\")\ntest_paths,  y_test  = collect_from_split(SPLIT_ROOT, \"test\")\n\nprint(f\"Images — train: {len(train_paths)}, val: {len(val_paths)}, test: {len(test_paths)}\")\n\n# ---------------------------------------------------------------\n# Embedding extraction\n# ---------------------------------------------------------------\ndef batch_embed(paths, batch_size=BATCH_SIZE):\n    embs = []\n    model.eval()\n    for i in tqdm(range(0, len(paths), batch_size), desc=\"Embedding batches\"):\n        batch = paths[i:i+batch_size]\n        imgs = []\n        for p in batch:\n            try:\n                imgs.append(transform(Image.open(p).convert(\"RGB\")))\n            except Exception as e:\n                print(\"skip:\", p, \"err:\", e)\n        if not imgs:\n            continue\n        x = torch.stack(imgs).to(DEVICE)\n        with torch.no_grad():\n            out = model(pixel_values=x)\n        embs.append(out.last_hidden_state[:,0,:].cpu().numpy())\n    return np.vstack(embs) if embs else np.zeros((0,0), dtype=np.float32)\n\n# ---------------------------------------------------------------\n# Save embeddings\n# ---------------------------------------------------------------\nif (OUT/\"X_train.npy\").exists():\n    print(\"Embeddings already exist — loading.\")\n    X_train = np.load(OUT/\"X_train.npy\"); y_train = np.load(OUT/\"y_train.npy\")\n    X_val   = np.load(OUT/\"X_val.npy\");   y_val   = np.load(OUT/\"y_val.npy\")\n    X_test  = np.load(OUT/\"X_test.npy\");  y_test  = np.load(OUT/\"y_test.npy\")\nelse:\n    print(\"Extracting train embeddings...\")\n    X_train = batch_embed(train_paths)\n    np.save(OUT/\"X_train.npy\", X_train); np.save(OUT/\"y_train.npy\", y_train)\n    print(\"Saved X_train\", X_train.shape)\n\n    print(\"Extracting val embeddings...\")\n    X_val = batch_embed(val_paths)\n    np.save(OUT/\"X_val.npy\", X_val); np.save(OUT/\"y_val.npy\", y_val)\n    print(\"Saved X_val\", X_val.shape)\n\n    print(\"Extracting test embeddings...\")\n    X_test = batch_embed(test_paths)\n    np.save(OUT/\"X_test.npy\", X_test); np.save(OUT/\"y_test.npy\", y_test)\n    print(\"Saved X_test\", X_test.shape)\n\nprint(\"Done. Shapes:\")\nprint(\"  Train:\", np.load(OUT/'X_train.npy').shape)\nprint(\"  Val:  \", np.load(OUT/'X_val.npy').shape)\nprint(\"  Test: \", np.load(OUT/'X_test.npy').shape)\nprint(\"Potato embeddings saved to:\", OUT.resolve())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T12:30:57.772583Z","iopub.execute_input":"2025-10-25T12:30:57.772901Z","iopub.status.idle":"2025-10-25T12:31:45.980559Z","shell.execute_reply.started":"2025-10-25T12:30:57.772878Z","shell.execute_reply":"2025-10-25T12:31:45.979788Z"}},"outputs":[{"name":"stdout","text":"Found 3 Potato disease classes:\n ['Early blight', 'Healthy', 'Late blight']\nImages — train: 1639, val: 350, test: 355\nExtracting train embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Embedding batches:   0%|          | 0/205 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42eb175f8ff246c78267a07a7ec10982"}},"metadata":{}},{"name":"stdout","text":"Saved X_train (1639, 768)\nExtracting val embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Embedding batches:   0%|          | 0/44 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1895fc9ff6e4496b3bc216e47168efe"}},"metadata":{}},{"name":"stdout","text":"Saved X_val (350, 768)\nExtracting test embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Embedding batches:   0%|          | 0/45 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beafc98b43534572aed995687badb515"}},"metadata":{}},{"name":"stdout","text":"Saved X_test (355, 768)\nDone. Shapes:\n  Train: (1639, 768)\n  Val:   (350, 768)\n  Test:  (355, 768)\nPotato embeddings saved to: /kaggle/working/embeddings_potato\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, normalize\nimport joblib\nimport os\n\nEMB_DIR = \"/kaggle/working/embeddings_potato\"\nMODEL_OUT = os.path.join(EMB_DIR, \"stage2_classifier_model_potato.joblib\")\n\n\n# load\nX_train = np.load(os.path.join(EMB_DIR, \"X_train.npy\"))\ny_train = np.load(os.path.join(EMB_DIR, \"y_train.npy\"))\nX_val   = np.load(os.path.join(EMB_DIR, \"X_val.npy\"))\ny_val   = np.load(os.path.join(EMB_DIR, \"y_val.npy\"))\n\nprint(\"Shapes loaded — X_train:\", X_train.shape, \"y_train:\", y_train.shape, \"X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n\n# basic sanity\nif X_train.size == 0 or X_val.size == 0:\n    raise SystemExit(\"Empty embeddings — check previous step. Exiting.\")\n\nif X_train.shape[0] != y_train.shape[0] or X_val.shape[0] != y_val.shape[0]:\n    raise SystemExit(\"Mismatch between number of embeddings and labels. Exiting.\")\n\n# Option A: Standardize (common). with_mean=True normally okay unless huge memmap / sparse\nsc = StandardScaler(with_mean=True, with_std=True)\nX_train_s = sc.fit_transform(X_train)\nX_val_s   = sc.transform(X_val)\n\n# Option B (alternative, often good for cosine-like): L2 normalize\n# X_train_s = normalize(X_train, norm='l2')\n# X_val_s   = normalize(X_val, norm='l2')\n\n# Train logistic regression (use class_weight='balanced' if classes imbalanced)\nclf = LogisticRegression(max_iter=2000, n_jobs=-1, C=1.0, class_weight=None, random_state=42)\nclf.fit(X_train_s, y_train)\n\n# Predict & evaluate\ny_pred = clf.predict(X_val_s)\nacc = accuracy_score(y_val, y_pred)\nprint(f\"Validation accuracy: {acc:.4f}\\n\")\nprint(\"Classification report:\\n\", classification_report(y_val, y_pred))\n\n# Save scaler + model\njoblib.dump((sc, clf), MODEL_OUT)\nprint(\"Saved model to:\", MODEL_OUT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T12:33:10.466039Z","iopub.execute_input":"2025-10-25T12:33:10.466324Z","iopub.status.idle":"2025-10-25T12:33:12.014347Z","shell.execute_reply.started":"2025-10-25T12:33:10.466303Z","shell.execute_reply":"2025-10-25T12:33:12.013480Z"}},"outputs":[{"name":"stdout","text":"Shapes loaded — X_train: (1639, 768) y_train: (1639,) X_val: (350, 768) y_val: (350,)\nValidation accuracy: 0.9629\n\nClassification report:\n               precision    recall  f1-score   support\n\n           0       0.95      0.97      0.96       164\n           1       1.00      1.00      1.00        22\n           2       0.97      0.95      0.96       164\n\n    accuracy                           0.96       350\n   macro avg       0.97      0.97      0.97       350\nweighted avg       0.96      0.96      0.96       350\n\nSaved model to: /kaggle/working/embeddings_potato/stage2_classifier_model_potato.joblib\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# 2 STAGE INFERENCE PIPELINE","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T13:06:05.985411Z","iopub.execute_input":"2025-10-25T13:06:05.985722Z","iopub.status.idle":"2025-10-25T13:06:06.091776Z","shell.execute_reply.started":"2025-10-25T13:06:05.985701Z","shell.execute_reply":"2025-10-25T13:06:06.090872Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2142328923.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Load encoder once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENCODER_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mevaluate_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFINAL_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{ROOT}/Stage_2_Splits\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/kaggle/working/encoder_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"],"ename":"FileNotFoundError","evalue":"[Errno 2] Unable to synchronously open file (unable to open file: name = '/kaggle/working/encoder_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)","output_type":"error"}],"execution_count":21}]}