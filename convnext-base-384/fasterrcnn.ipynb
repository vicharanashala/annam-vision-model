{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13519043,"sourceType":"datasetVersion","datasetId":8583745}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/pratikkayal/PlantDoc-Object-Detection-Dataset.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:02:56.707265Z","iopub.execute_input":"2025-10-27T08:02:56.707438Z","iopub.status.idle":"2025-10-27T08:03:27.231443Z","shell.execute_reply.started":"2025-10-27T08:02:56.707422Z","shell.execute_reply":"2025-10-27T08:03:27.230734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.transforms import functional as TF\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:24:42.171564Z","iopub.execute_input":"2025-10-27T08:24:42.171809Z","iopub.status.idle":"2025-10-27T08:24:42.175589Z","shell.execute_reply.started":"2025-10-27T08:24:42.171793Z","shell.execute_reply":"2025-10-27T08:24:42.174891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef print_directory_sample(root_dir, indent=\"\"):\n    items = sorted(os.listdir(root_dir))\n    dirs = [item for item in items if os.path.isdir(os.path.join(root_dir, item))]\n    files = [item for item in items if os.path.isfile(os.path.join(root_dir, item))]\n    \n    # Print up to 10 files\n    for fname in files[:10]:\n        print(indent + \"|-- \" + fname)\n    if len(files) > 10:\n        print(indent + \"    ...\")\n    \n    # Recursively print directories\n    for d in dirs:\n        print(indent + \"|-- \" + d + \"/\")\n        print_directory_sample(os.path.join(root_dir, d), indent + \"    \")\n\n# Set your dataset root, e.g., \".\"\nroot_path = \".\"\nprint_directory_sample(root_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T08:24:42.427735Z","iopub.execute_input":"2025-10-27T08:24:42.427921Z","iopub.status.idle":"2025-10-27T08:24:42.487049Z","shell.execute_reply.started":"2025-10-27T08:24:42.427907Z","shell.execute_reply":"2025-10-27T08:24:42.486464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# Dataset path setup\nDATASET_PATH = \"PlantDoc-Object-Detection-Dataset\"\nTRAIN_IMG_DIR = os.path.join(DATASET_PATH, \"TRAIN\")\nTEST_IMG_DIR = os.path.join(DATASET_PATH, \"TEST\")\nTRAIN_CSV = os.path.join(DATASET_PATH, \"train_labels.csv\")\nTEST_CSV = os.path.join(DATASET_PATH, \"test_labels.csv\")\n\ndef load_annotations(csv_path):\n    df = pd.read_csv(csv_path)\n    return df[['filename', 'width', 'height', 'xmin', 'ymin', 'xmax', 'ymax', 'class']]\n\ntrain_df = load_annotations(TRAIN_CSV)\ntest_df = load_annotations(TEST_CSV)\n\nclass LeafDataset(Dataset):\n    def __init__(self, df, img_folder, transforms=None):\n        self.df = df\n        self.img_folder = img_folder\n        self.transforms = transforms\n        self.imgs = df['filename'].unique()\n        \n        # Filter valid images\n        self.valid_imgs = []\n        print(f\"Validating images in {img_folder}...\")\n        for img_name in self.imgs:\n            img_path = os.path.join(img_folder, img_name)\n            if os.path.exists(img_path):\n                try:\n                    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n                    if img is not None:\n                        self.valid_imgs.append(img_name)\n                    else:\n                        print(f\"Skipping corrupted: {img_name}\")\n                except:\n                    print(f\"Skipping unreadable: {img_name}\")\n            else:\n                print(f\"Skipping missing: {img_name}\")\n        \n        self.imgs = self.valid_imgs\n        print(f\"Valid images: {len(self.imgs)}/{len(df['filename'].unique())}\\n\")\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, idx):\n        img_name = self.imgs[idx]\n        img_path = os.path.join(self.img_folder, img_name)\n\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        if img is None:\n            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n            if img is None:\n                raise ValueError(f\"Image not found: {img_path}\")\n\n        # Convert to RGB\n        if len(img.shape) == 2:\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n        elif len(img.shape) == 3:\n            if img.shape[2] == 1:\n                img = cv2.cvtColor(img.squeeze(), cv2.COLOR_GRAY2RGB)\n            elif img.shape[2] == 2:\n                img = cv2.cvtColor(img[:, :, 0], cv2.COLOR_GRAY2RGB)\n            elif img.shape[2] == 3:\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            elif img.shape[2] == 4:\n                img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGB)\n\n        img = np.ascontiguousarray(img, dtype=np.uint8)\n        assert img.shape[2] == 3, f\"Expected 3 channels, got {img.shape}\"\n\n        if self.transforms:\n            img = self.transforms(img)\n        \n        assert img.shape[0] == 3, f\"Expected 3 channels after transform, got {img.shape[0]}\"\n\n        boxes = []\n        labels = []\n        for _, row in self.df[self.df['filename'] == img_name].iterrows():\n            boxes.append([row['xmin'], row['ymin'], row['xmax'], row['ymax']])\n            labels.append(1)\n\n        boxes = torch.tensor(boxes, dtype=torch.float32)\n        labels = torch.tensor(labels, dtype=torch.int64)\n        target = {'boxes': boxes, 'labels': labels}\n\n        return img, target\n\ndef get_transform():\n    return T.Compose([T.ToTensor()])\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n# Create datasets\ntrain_dataset = LeafDataset(train_df, TRAIN_IMG_DIR, transforms=get_transform())\ntest_dataset = LeafDataset(test_df, TEST_IMG_DIR, transforms=get_transform())\n\n# Create dataloaders\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=4, \n    shuffle=True, \n    collate_fn=collate_fn,\n    num_workers=2,\n    pin_memory=True\n)\n\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=1, \n    shuffle=False, \n    collate_fn=collate_fn\n)\n\n# Model setup\nprint(\"Setting up model...\")\nweights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\nmodel = fasterrcnn_resnet50_fpn(weights=weights)\n\n# Replace box predictor\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=2)\n\n# Device setup - SINGLE GPU ONLY (no DataParallel)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nprint(f\"Using device: {device}\")\nif torch.cuda.device_count() > 1:\n    print(f\"Note: {torch.cuda.device_count()} GPUs available, but using single GPU\")\n    print(\"DataParallel is not compatible with Faster R-CNN in this setup\")\n\noptimizer = torch.optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()), \n    lr=5e-4\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T09:30:58.898850Z","iopub.execute_input":"2025-10-27T09:30:58.899599Z","iopub.status.idle":"2025-10-27T09:31:29.560842Z","shell.execute_reply.started":"2025-10-27T09:30:58.899575Z","shell.execute_reply":"2025-10-27T09:31:29.560011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nnum_epochs = 10\nprint(f\"\\nStarting training for {num_epochs} epochs...\\n\")\n\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n    num_batches = 0\n    \n    for batch_idx, (images, targets) in enumerate(train_loader):\n        # Move to device\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        # Forward pass\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        \n        # Backward pass\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        epoch_loss += losses.item()\n        num_batches += 1\n        \n        # Print progress\n        if batch_idx % 50 == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {losses.item():.4f}\")\n    \n    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0\n    print(f\"\\n{'='*60}\")\n    print(f\"Epoch [{epoch + 1}/{num_epochs}] Complete - Average Loss: {avg_loss:.4f}\")\n    print(f\"{'='*60}\\n\")\n\nprint(\"Training complete!\")\n\n# Save model\ntorch.save(model.state_dict(), 'leaf_disease_detector.pth')\nprint(\"Model saved as 'leaf_disease_detector.pth'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T09:31:29.562001Z","iopub.execute_input":"2025-10-27T09:31:29.562319Z","iopub.status.idle":"2025-10-27T11:44:34.173279Z","shell.execute_reply.started":"2025-10-27T09:31:29.562294Z","shell.execute_reply":"2025-10-27T11:44:34.172482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport torchvision.transforms as T\n\n# Load the trained model\ndef load_model(model_path, num_classes=2, device='cuda:0'):\n    \"\"\"Load the trained Faster R-CNN model\"\"\"\n    model = fasterrcnn_resnet50_fpn(weights=None)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    \n    # Load weights\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()\n    \n    print(f\"Model loaded from {model_path}\")\n    return model\n\n# Preprocess image\ndef preprocess_image(image_path):\n    \"\"\"Load and preprocess image for inference\"\"\"\n    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n    if img is None:\n        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n    \n    if img is None:\n        raise ValueError(f\"Could not load image: {image_path}\")\n    \n    # Convert to RGB\n    if len(img.shape) == 2:\n        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    elif len(img.shape) == 3:\n        if img.shape[2] == 1:\n            img = cv2.cvtColor(img.squeeze(), cv2.COLOR_GRAY2RGB)\n        elif img.shape[2] == 2:\n            img = cv2.cvtColor(img[:, :, 0], cv2.COLOR_GRAY2RGB)\n        elif img.shape[2] == 3:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        elif img.shape[2] == 4:\n            img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGB)\n    \n    img = np.ascontiguousarray(img, dtype=np.uint8)\n    \n    # Convert to tensor\n    transform = T.Compose([T.ToTensor()])\n    img_tensor = transform(img)\n    \n    return img, img_tensor\n\n# Perform inference\ndef predict(model, image_tensor, device='cuda:0', confidence_threshold=0.75):\n    \"\"\"Run inference on an image\"\"\"\n    with torch.no_grad():\n        image_tensor = image_tensor.to(device)\n        predictions = model([image_tensor])\n    \n    # Filter by confidence\n    pred = predictions[0]\n    keep = pred['scores'] > confidence_threshold\n    \n    boxes = pred['boxes'][keep].cpu().numpy()\n    scores = pred['scores'][keep].cpu().numpy()\n    labels = pred['labels'][keep].cpu().numpy()\n    \n    return boxes, scores, labels\n\n# Visualize predictions\ndef visualize_predictions(image, boxes, scores, labels, title=\"Predictions\", save_path=None):\n    \"\"\"Visualize bounding boxes on image\"\"\"\n    fig, ax = plt.subplots(1, figsize=(12, 8))\n    ax.imshow(image)\n    \n    # Draw each bounding box\n    for box, score, label in zip(boxes, scores, labels):\n        xmin, ymin, xmax, ymax = box\n        width = xmax - xmin\n        height = ymax - ymin\n        \n        # Create rectangle\n        rect = patches.Rectangle(\n            (xmin, ymin), width, height,\n            linewidth=2, edgecolor='red', facecolor='none'\n        )\n        ax.add_patch(rect)\n        \n        # Add label with confidence\n        label_text = f'Leaf: {score:.2f}'\n        ax.text(\n            xmin, ymin - 5,\n            label_text,\n            color='white',\n            fontsize=10,\n            bbox=dict(facecolor='red', alpha=0.7, edgecolor='none', pad=2)\n        )\n    \n    ax.set_title(f\"{title} - {len(boxes)} detections\", fontsize=14, fontweight='bold')\n    ax.axis('off')\n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        print(f\"Saved visualization to {save_path}\")\n    \n    plt.show()\n\n# Test on a single image\ndef test_single_image(model, image_path, device='cuda:0', confidence_threshold=0.75, save_path=None):\n    \"\"\"Test model on a single image\"\"\"\n    print(f\"\\nTesting on: {image_path}\")\n    \n    # Load and preprocess\n    image, image_tensor = preprocess_image(image_path)\n    \n    # Predict\n    boxes, scores, labels = predict(model, image_tensor, device, confidence_threshold)\n    \n    print(f\"Detected {len(boxes)} objects\")\n    for i, (box, score) in enumerate(zip(boxes, scores)):\n        print(f\"  Detection {i+1}: Confidence={score:.3f}, Box={box.astype(int)}\")\n    \n    # Visualize\n    visualize_predictions(\n        image, boxes, scores, labels, \n        title=os.path.basename(image_path),\n        save_path=save_path\n    )\n    \n    return boxes, scores, labels\n\n# Test on multiple images\ndef test_multiple_images(model, image_paths, device='cuda:0', confidence_threshold=0.5):\n    \"\"\"Test model on multiple images\"\"\"\n    results = []\n    \n    for img_path in image_paths:\n        try:\n            boxes, scores, labels = test_single_image(\n                model, img_path, device, confidence_threshold\n            )\n            results.append({\n                'path': img_path,\n                'boxes': boxes,\n                'scores': scores,\n                'labels': labels,\n                'num_detections': len(boxes)\n            })\n        except Exception as e:\n            print(f\"Error processing {img_path}: {e}\")\n            results.append({\n                'path': img_path,\n                'error': str(e)\n            })\n    \n    return results\n\n# Test on test dataset\ndef test_on_test_set(model, test_loader, device='cuda:0', num_samples=5):\n    \"\"\"Test on images from test dataset\"\"\"\n    model.eval()\n    \n    print(f\"\\nTesting on {num_samples} random test images...\\n\")\n    \n    for i, (images, targets) in enumerate(test_loader):\n        if i >= num_samples:\n            break\n        \n        image_tensor = images[0].to(device)\n        target = targets[0]\n        \n        # Get predictions\n        with torch.no_grad():\n            predictions = model([image_tensor])\n        \n        pred = predictions[0]\n        \n        # Convert tensors to numpy\n        image_np = image_tensor.cpu().permute(1, 2, 0).numpy()\n        pred_boxes = pred['boxes'].cpu().numpy()\n        pred_scores = pred['scores'].cpu().numpy()\n        pred_labels = pred['labels'].cpu().numpy()\n        gt_boxes = target['boxes'].cpu().numpy()\n        \n        # Filter predictions by confidence\n        keep = pred_scores > 0.75\n        pred_boxes = pred_boxes[keep]\n        pred_scores = pred_scores[keep]\n        \n        # Visualize predictions vs ground truth\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n        \n        # Ground truth\n        ax1.imshow(image_np)\n        for box in gt_boxes:\n            xmin, ymin, xmax, ymax = box\n            rect = patches.Rectangle(\n                (xmin, ymin), xmax-xmin, ymax-ymin,\n                linewidth=2, edgecolor='green', facecolor='none'\n            )\n            ax1.add_patch(rect)\n        ax1.set_title(f\"Ground Truth ({len(gt_boxes)} boxes)\", fontsize=12, fontweight='bold')\n        ax1.axis('off')\n        \n        # Predictions\n        ax2.imshow(image_np)\n        for box, score in zip(pred_boxes, pred_scores):\n            xmin, ymin, xmax, ymax = box\n            rect = patches.Rectangle(\n                (xmin, ymin), xmax-xmin, ymax-ymin,\n                linewidth=2, edgecolor='red', facecolor='none'\n            )\n            ax2.add_patch(rect)\n            ax2.text(\n                xmin, ymin - 5, f'{score:.2f}',\n                color='white', fontsize=9,\n                bbox=dict(facecolor='red', alpha=0.7, edgecolor='none')\n            )\n        ax2.set_title(f\"Predictions ({len(pred_boxes)} boxes)\", fontsize=12, fontweight='bold')\n        ax2.axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n        \n        print(f\"Image {i+1}: GT boxes={len(gt_boxes)}, Predicted boxes={len(pred_boxes)}\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:55:20.642821Z","iopub.execute_input":"2025-10-27T12:55:20.643123Z","iopub.status.idle":"2025-10-27T12:55:20.665292Z","shell.execute_reply.started":"2025-10-27T12:55:20.643102Z","shell.execute_reply":"2025-10-27T12:55:20.664645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ====================\n# MAIN TESTING CODE\n# ====================\n\n# Load the trained model\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = load_model('leaf_disease_detector.pth', num_classes=2, device=device)\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:13:09.687061Z","iopub.execute_input":"2025-10-27T12:13:09.687804Z","iopub.status.idle":"2025-10-27T12:13:10.523043Z","shell.execute_reply.started":"2025-10-27T12:13:09.687782Z","shell.execute_reply":"2025-10-27T12:13:10.522424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Option 1: Test on specific image files\n\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"OPTION 1: Test on specific images\")\nprint(\"=\"*60)\n\n# Option 1: Test on specific image files\nprint(\"\\n\" + \"=\"*60)\nprint(\"OPTION 1: Test on specific images\")\nprint(\"=\"*60)\n\n# Example: Test on images from TEST directory\ntest_images = [\n    \"/kaggle/input/field-like/test-samples/potato early blight/Pasted image (5).png\"\n]\n# Filter to only existing images\ntest_images = [img for img in test_images if os.path.exists(img)]\n\nif test_images:\n    for img_path in test_images:\n        test_single_image(model, img_path, device, confidence_threshold=0.75)\nelse:\n    print(\"No test images found. Please provide valid image paths.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:55:28.450399Z","iopub.execute_input":"2025-10-27T12:55:28.451228Z","iopub.status.idle":"2025-10-27T12:55:29.034401Z","shell.execute_reply.started":"2025-10-27T12:55:28.451203Z","shell.execute_reply":"2025-10-27T12:55:29.033603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Option 2: Test on test dataset loader\nprint(\"\\n\" + \"=\"*60)\nprint(\"OPTION 2: Test on test dataset\")\nprint(\"=\"*60)\n\n# Load test dataset (assuming test_loader is already created)\ntry:\n    test_on_test_set(model, test_loader, device, num_samples=5)\nexcept NameError:\n    print(\"test_loader not found. Please create it first using the training script.\")\n\nprint(\"\\nTesting complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T12:13:13.944315Z","iopub.execute_input":"2025-10-27T12:13:13.944591Z","iopub.status.idle":"2025-10-27T12:13:18.197035Z","shell.execute_reply.started":"2025-10-27T12:13:13.944570Z","shell.execute_reply":"2025-10-27T12:13:18.196221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef test_single_image(model, image_path, device='cuda:0', confidence_threshold=0.75, save_path=None, crop_output_dir=None):\n    \"\"\"Test model on a single image, display boxes, and crop detections\"\"\"\n    print(f\"\\nTesting on: {image_path}\")\n    \n    # Load and preprocess\n    image, image_tensor = preprocess_image(image_path)\n    \n    # Predict\n    boxes, scores, labels = predict(model, image_tensor, device, confidence_threshold)\n    \n    print(f\"Detected {len(boxes)} objects\")\n    for i, (box, score, label) in enumerate(zip(boxes, scores, labels)):\n        print(f\"  Detection {i+1}: Label={label}, Confidence={score:.3f}, Box={box.astype(int)}\")\n    \n    # Draw bounding boxes on the original image\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image)\n    for box, score, label in zip(boxes, scores, labels):\n        x1, y1, x2, y2 = box\n        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n                             fill=False, color='lime', linewidth=2)\n        plt.gca().add_patch(rect)\n        plt.text(x1, y1 - 5, f\"{label} {score:.2f}\",\n                 color='yellow', fontsize=12, backgroundcolor=\"black\")\n    plt.axis(\"off\")\n    plt.title(os.path.basename(image_path))\n    plt.show()\n\n    # Crop and save/show each detection\n    if crop_output_dir:\n        os.makedirs(crop_output_dir, exist_ok=True)\n\n    pil_img = Image.open(image_path).convert(\"RGB\")\n    cropped_images = []\n\n    for i, (box, score, label) in enumerate(zip(boxes, scores, labels)):\n        x1, y1, x2, y2 = map(int, box)\n        cropped = pil_img.crop((x1, y1, x2, y2))\n        cropped_images.append(cropped)\n\n        # Show each cropped object\n        plt.figure()\n        plt.imshow(cropped)\n        plt.title(f\"{label} ({score:.2f})\")\n        plt.axis(\"off\")\n        plt.show()\n\n        # Optionally save cropped images\n        if crop_output_dir:\n            crop_filename = f\"{label}_{i+1}_{score:.2f}.png\"\n            crop_path = os.path.join(crop_output_dir, crop_filename)\n            cropped.save(crop_path)\n            print(f\"Saved crop: {crop_path}\")\n\n    return boxes, scores, labels, cropped_images\n\n\n# Example usage\ntest_images = [\n    \"/kaggle/input/field-like/test-samples/potato early blight/Pasted image (5).png\"\n]\n\n# Filter to only existing images\ntest_images = [img for img in test_images if os.path.exists(img)]\n\nif test_images:\n    for img_path in test_images:\n        test_single_image(\n            model, img_path, device,\n            confidence_threshold=0.75,\n            crop_output_dir=\"cropped_detections\"\n        )\nelse:\n    print(\"No test images found. Please provide valid image paths.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T13:03:15.519652Z","iopub.execute_input":"2025-10-27T13:03:15.520421Z","iopub.status.idle":"2025-10-27T13:03:16.561546Z","shell.execute_reply.started":"2025-10-27T13:03:15.520395Z","shell.execute_reply":"2025-10-27T13:03:16.560914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}